# Isolated-sign-language-recognition:
Deep Learning project on the Google- Isolated Sign Language Recognition competition on Kaggle

- Notebook: https://github.com/pradhyumn/Isolated-sign-language-recognition/blob/main/main.ipynb
- Model: https://drive.google.com/file/d/10NET2mHXFrS5CBxxnASp4zXRaa9u_sz9/view?usp=share_link
- Kaggle Link: https://www.kaggle.com/code/pradhyumnbhale/transformer-model-training


# Project Description:
The goal of this project is to develop a deep learning model for real-time isolated sign language recognition in the PopSign smartphone app, making it more interactive and improving learning outcomes for users who want to learn American Sign Language (ASL) to communicate with deaf individuals. Our aim was to publish our approach on the respective [Kaggle Competition](https://www.kaggle.com/competitions/asl-signs). 

# Dataset:
We will be using the Isolated Sign Language Recognition corpus (version 1.0) dataset, which contains hand and facial landmarks generated by Mediapipe from approximately 100k videos of isolated signs. The dataset comprises signs performed by 21 deaf signers from a 250-sign vocabulary.

## Feature Statistics:
After the pre-processing of the frames, we show the *features statistics* of Lips, Hands and Pose that we extracted from the data in the following Boxplots.

<p float="left">
  <img src="/Figures/Lips.png" width="250" display="inline" />
  <img src="/Figures/Pose.png" width="250" display="inline" />
  <img src="/Figures/Hands.png" width="250" display="inline" />
</p>



# Methodology:

## Model Design
We needed to implement transformer from scratch as TFLite does not support the native TF implementation of MultiHeadAttention. The MultiHeadAttention layer is a key component of the Transformer model, and it may not have been possible to use a pre-existing Transformer implementation in TensorFlow in TFLite if the implementation is not supported by TFLite. Therefore, we implemented the code for Transformer as well instead of relying on the implementation of the MultiHeadAttention layer in TensorFlow.

![Alt text](/Figures/Transformer_block.png)

## Main Architecture
The main component in our code pertains to the implementation of the fundamental component of the Transformer model, the **Multi-Head Attention mechanism** as discussed earlier. This mechanism concentrates on different segments of the input sequence, hence enhancing the model's ability to capture various aspects of the information. The code consists of two primary parts. The *scaled dot-product* function calculates the scaled dot product attention, which is then used in the *MultiHeadAttention* class to compute the multi-head attention output. The *MultiHeadAttention* class is a custom *Keras* layer which divides the input into multiple *heads* and applies the scaled dot product attention to each. These individual attention outputs are then concatenated and processed to generate the final multi-head attention output. The code is crucial for Transformer models, enabling them to handle long sequence data and complex patterns, contributing significantly to tasks like Sign Language Recognition. The *scaled dot-product* attention used for the attention mechanism is a method for calculating Attention weights. In this method, the dot product of Query and Key is divided by a scaling factor, then normalized by the softmax function, and finally multiplied by Value to obtain the Attention output.

## Model
The implemented TensorFlow model in this code takes two inputs, namely *frames* and *non empty frame indexes*. The *frames* input represents video data composed of multiple frames, while *non empty frame indexes* indicates which frames contain content. By applying masking, the model focuses on locations with valid frames for training. Utilizing the Transformer architecture, the model processes input through a combination of attention mechanisms and multiple layers. Each frame is embedded into three distinct representations: LIPS, LEFT HAND, and POSE, which form the input for the Transformer. Additionally, the model incorporates various techniques, such as random frame masking, category loss, and label smoothing. The model includes an AdamW optimizer and several evaluation metrics, such as sparse classification accuracy and top-k accuracy for sparse classification. More details about this in the next section.

# Training Strategy
We trained the models using a GPU-accelerated environment to speed up the process. To find the best model configuration, we increased the number of training epochs to allow the models more opportunities to learn from the dataset. The architecture summary for the model parameters is detailed in the following tables. The hyperparameter values for our best model are also given in the following tables.

### Architecture Summary

| **Metric**                 | **Count**    |
|----------------------------|--------------|
| Total Parameters           | 7,415,165    |
| Trainable Parameters       | 7,415,165    |
| Non-trainable Parameters   | 0            |


### Hyperparameters

| **Hyperparameter**                    | **Value/Type**                           |
|---------------------------------------|-------------------------------------------|
| Loss                                  | Sparse Categorical Cross-entropy          |
| Optimizer                             | Adam with weight decay                    |
| Batch Size                            | 512                                       |
| LR Scheduler                          | Custom                                    |
| LR Max                                | 0.00092                                   |
| Weight Decay                          | 1e-5                                      |
| WD Ratio                              | 0.054                                     |
| Mask Value                            | 4237                                      |
| Epochs                                | 200                                       |

![Alt text](/Figures/encoder-decoder-2.jpg)


# Model Architecture

- The model takes in two inputs: non\_empty\_frame\_idxs and frames. It seems like frames represent a sequence of 64 frames, each frame being a 66x3 matrix, possibly a form of image data or time-series data.
- The non\_empty\_frame\_idxs are used to identify valid or non-empty frames in the sequence. This information is used to process parts of the frame's data selectively.
- The model performs a series of transformations on the frames data, including slicing, reshaping, and mathematical operations. These transformations appear to be custom operations designed specifically for this dataset and task.
- The processed frame data is then passed through an Embedding layer, which maps each frame to a higher-dimensional space of size 512. This is a common operation in natural language processing models, where it's used to map words or tokens to a continuous vector space.
- The output of the Embedding layer is then passed through a Transformer layer. Transformers are a type of model that uses self-attention mechanisms to process sequence data. They are very powerful and flexible and can handle long sequences and complex dependencies between different parts of the sequence.
- The output of the Transformer layer is then averaged and passed through a Dropout layer for regularization, which helps prevent overfitting.
- Finally, the output of the Dropout layer is passed through a Dense layer with 250 units, which performs a linear transformation of the data.

# Results
Our model performance and evaluation results are tabulated in the following tables. Our results are calculated on our TFLite model. Specifically, we achieve high scores for a lot of the class but we showcase the classes our model falters on. Finally, we reduced our model size from 28MB to 7MB while converting to TFLite. We observed a latency of around 22 frames per second while testing our model. However, the model has been tested on the Greene HPC, and may perform worse on a mobile phone.

### Model Evaluation

| **Metric**        | **Value**     |
|-------------------|---------------|
| Loss              | 1.9537        |
| Accuracy          | 0.99          |
| Top 5 Accuracy    | 1.000         |
| Top 10 Accuracy   | 1.000         |
| Learning Rate     | 3.7817e-08    |
| Total Epochs      | 245           |

### Results

|               | **Precision** | **Recall** | **F1-Score** |
|---------------|---------------|------------|--------------|
| Wet           | 1.00          | 0.95       | 0.98         |
| Bath          | 0.95          | 1.00       | 0.98         |
| Story         | 1.00          | 0.97       | 0.98         |
| Sun           | 1.00          | 0.97       | 0.98         |
| Aunt          | 0.96          | 1.00       | 0.98         |
| Many          | 0.89          | 1.00       | 0.94         |
| Stay          | 0.88          | 1.00       | 0.94         |
| Accuracy      | 0.99          | 0.99       | 0.99         |
| Macro Avg     | 0.99          | 0.99       | 0.99         |
| Weighted Avg  | 0.99          | 0.99       | 0.99         |

### Importance of Different Landmarks in ASL Classification

| **Landmark**             | **Weight (%)** |
|--------------------------|----------------|
| Lip Movements            | 29.7           |
| Left Hand Movements      | 44.0           |
| Overall Body Posture     | 26.2           |


# Conclusion
In this discussion, we delved into several aspects pertinent to Sign Language Recognition (SLR), focusing on the value of landmark or key point embeddings and the role of transformer models in this context. Landmark or key point embeddings, essential for dimensionality reduction, feature extraction, noise reduction, and providing semantic similarity, were acknowledged as a vital tool for converting raw sign language data into a more digestible form for machine learning models, thereby enhancing the SLR's efficiency and accuracy.

A custom implementation of the Transformer architecture was also examined. The model, designed to process inputs through multiple attention mechanisms, emphasized the importance of considering both spatial and temporal features in sign language data. Notably, the model embedded each frame into three different representations, LIPS, LEFT HAND, and POSE, which were then fed to the Transformer. Additionally, the model incorporated various techniques such as random frame masking, category loss, and label smoothing, highlighting the diverse strategies used in tackling the SLR challenge.
Lastly, the importance of custom learning rate schedulers, especially for tasks like SLR, was discussed. 

In conclusion, the successful application of machine learning techniques to SLR involves careful consideration of various components, including the use of appropriate embeddings, attention-based models like transformers, and custom learning rate schedulers. These techniques, when applied judiciously, can significantly enhance the model's ability to accurately recognize sign language, opening up new possibilities for communication and accessibility.





